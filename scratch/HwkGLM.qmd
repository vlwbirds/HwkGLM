---
title: "HwkGLM"
format: html
editor: visual
---

## Model Hawk GLM

This is an attempt to build code for a generalize linear model. The data used are predator-prey reaction behaviors collected by launching a model hawk at birds in the Amazon in Madre de Dios, Peru. Reactions include: no response, call, alarm, sing, dive, hide, freeze, visual search

### Installing Packages

```{r}
#install.packages("glmm")
#install.packages("here")
#install.packages("lme4")
#install.packages("Matrix")
#install.packages("MuMIn")
#install.packages("performance")
#install.packages("tidyverse")

library(glmm)
library(here)
library(lme4)
library(Matrix)
library(MuMIn)
library(performance)
library(tidyverse)

```

## Loading Hawk Data

```{r}
hwk_GLM <- read_csv(here("data/hwkGLM.csv"))
view(hwk_GLM)

hwk_data <- read_csv(here("data/hwkdata.csv")) %>% 
  mutate(Flock_size = as.numeric(Flock_size)) %>% 
  mutate(Post_Dist = as.numeric(Post_Dist)) %>% 
  mutate(Veg_dense_under_avg = as.numeric(Veg_dense_under_avg)) %>% 
  mutate(Veg_under_height = as.numeric(Veg_under_height)) %>% 
  mutate(Veg_mid_low = as.numeric(Veg_mid_low)) %>% 
  mutate(Veg_mid_high = as.numeric(Veg_mid_high)) %>% 
  mutate(Veg_can_low = as.numeric(Veg_can_low)) %>% 
  mutate(Veg_can_high = as.numeric(Veg_can_high)) %>% 
  mutate(Rad_Avg_Dense = as.numeric(Rad_Avg_Dense)) %>% 
  mutate(across(c(Social_Index, hwk, Species, Mid_Vocal, Mid_Behav_2, Mid_Join, Sex, sexID, SppID), factor))
         #across(c(Veg_mid_low, Veg_mid_high, Veg_can_low, Veg_can_high, Rad_Avg_Dense), numeric))
  
hwk_data$Social_Index [hwk_data$Social_Index == "#N/A"] <- NA
hwk_data$Veg_dense_under_avg [hwk_data$Veg_dense_under_avg == "na" ] <- NA
str(hwk_data)


#view(hwk_data)

#fake data
hwk_fake <- read_csv(here("data/hwk_fake.csv")) %>% 
  mutate(Social_Index = as.factor(Social_Index))

#fake data

hwk_fake2 <- read_csv(here("data/hwk_fake_comp1.csv")) %>% 
  mutate(Social_Index = as.factor(Social_Index))
```

## Generalized Linear Model 1 (larger index)

```{r}
model1 <- glm(Mid_Join_Index1 ~ Mid_Join, data = hwk_GLM, family = poisson(link = "log"))
summary(model1)
```

## Generalized Linear Model 2 (conservative)

```{r}
#remove competitive
hwk_glm_1 <- hwk_GLM[hwk_GLM$Social_Index != "2", ]

model2 <- glm(Mid_Join_Index2 ~ Social_Index + SpeciesID + Pre_Height + Pre_Dense, data = hwk_glm_1, family = poisson(link = "log"))
summary(model2)

model3 <- glm(alarm ~ Social_Index, data = hwk_glm_1, family = binomial(link = "logit"))
summary(model3)
view(hwk_glm_1)
# update to multinomial distrubution
```

response \~ species + veg density + foraging + sociality

Sociality: MSF, singles, pairs, leks, SSF (single species group)

## Create Alarm in Dataframes

```{r}
hwk_GLM$alarm <- ifelse(hwk_GLM$Mid_Join_Index2 == 4,1,0)
View(hwk_GLM)

hwk_data$alarm <- ifelse(hwk_data$Mid_Vocal_Index == 2,1,0)
view(hwk_data)
```

## Social Index GLM Model

```{r}

model <- glm(alarm ~ Social_Index + SpeciesID + Pre_Height + Pre_Dense, data = hwk_GLM, family = binomial(link = "logit"))
summary(model)
```

```{r}
model <- glm(alarm ~ Social_Index + 
               SpeciesID + 
               Pre_Height + 
               Pre_Dense, 
             data = hwk_GLM, family = binomial(link = "logit"))
summary(model)
```

## Modeling with full hwk_data

```{r}
# selecting only hwk treatment
hwk_subset <- subset(hwk_data, hwk == 1)
hwk_subset$alarm <- as.factor(ifelse(hwk_subset$Mid_Vocal_Index==2,1,0))

# GLM model
model <- glm(alarm ~ SppID +
               sexID + 
               Flock_size + 
              # Flight_proximity + 
               #Pre_Height + 
               #Veg_under_height + 
              # Veg_mid_low + 
               #Veg_mid_high + 
              # Rad_Avg_Dense + 
               Pre_Dense, 
             data = hwk_subset, family = binomial(link = "logit"))
summary(model)
```

## Treatment Significance - Control v Hawk

```{r}
trt <- glm(alarm ~ hwk, data = hwk_data, family = binomial(link = "logit"))
summary(trt)             
```

## Bar Plot of Treatment Versus Control Alarm Percentage

```{r}
# Calculate percentage of positive reactions for treatment and control
percentage_treatment <- mean(hwk_data$alarm[hwk_data$hwk == 1]) * 100
percentage_control <- mean(hwk_data$alarm[hwk_data$hwk == 0]) * 100

# Calculate sample size for treatment and control
sample_size_treatment <- sum(hwk_data$hwk == 1)
sample_size_control <- sum(hwk_data$hwk == 0)

# Create a new data frame for plotting
plot_data <- data.frame(
  Group = c("Treatment", "Control"),
  Percentage = c(percentage_treatment, percentage_control),
  SampleSize = c(sample_size_treatment, sample_size_control)
)

# Create a bar plot
ggplot(plot_data, aes(x = Group, y = Percentage, fill = Group)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = paste("n =", SampleSize)), position = position_dodge(width = 0.9), vjust = -0.5) +
  labs(title = "Percentage of Positive Reactions by Group",
       y = "Alarm Percentage",
       x = "Group") +
  scale_y_continuous(labels = scales::percent_format(scale = 1), limits = c(0, 50)) +
  theme_minimal()


```

## Social Structure Alarm Plot

```{r}
#subsetting for only treatment
hwk_subset <- subset(hwk_data, hwk == 1)
hwk_subset$alarm <- as.numeric(ifelse(hwk_subset$Mid_Vocal_Index==2,1,0))

# percentages of alarm by social structure
percent_solo <- mean(hwk_subset$alarm[hwk_subset$Social_Index == 0], na.rm = TRUE) * 100
percent_pair <- mean(hwk_subset$alarm[hwk_subset$Social_Index == 1], na.rm = TRUE) * 100
percent_comp <- mean(hwk_subset$alarm[hwk_subset$Social_Index == 2], na.rm = TRUE) * 100
percent_ssf <- mean(hwk_subset$alarm[hwk_subset$Social_Index == 3], na.rm = TRUE) * 100
percent_msf <- mean(hwk_subset$alarm[hwk_subset$Social_Index == 4], na.rm = TRUE) * 100

# Sample sizes
sample_solo <- sum(hwk_subset$Social_Index == 0, na.rm = TRUE)
sample_pair <- sum(hwk_subset$Social_Index == 1, na.rm = TRUE)
sample_comp <- sum(hwk_subset$Social_Index == 2, na.rm = TRUE)
sample_ssf <- sum(hwk_subset$Social_Index == 3, na.rm = TRUE)
sample_msf <- sum(hwk_subset$Social_Index == 4, na.rm = TRUE)

# New dataframe for plotting
plot_data <- data.frame(
 Group = factor(c("Solo", "Pair", "Single-species Flock", "Mixed-species Flock", "Competitive"),
                levels = c("Solo", "Pair", "Single-species Flock", "Mixed-species Flock", "Competitive")),  
 Percentage = c(percent_solo, percent_pair, percent_ssf, percent_msf, percent_comp),
  SampleSize = c(sample_solo, sample_pair, sample_ssf, sample_msf, sample_comp)
)

# Create a bar plot
ggplot(plot_data, aes(x = Group, y = Percentage, fill = Group)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = paste("n =", SampleSize)), position = position_dodge(width = 0.9), vjust = -0.5) +
  labs(title = "Percentage of Positive Reactions by Sociality",
       y = "Alarm Percentage",
       x = "Group") +
  scale_y_continuous(labels = scales::percent_format(scale = 1), limits = c(0, 50)) +
  theme_minimal()
```

## Social Structure By Species Alarming

```{r}
alarm_subset <- subset(hwk_subset, alarm == 1)
social_group <- alarm_subset %>% 
  group_by(Social_Index, Species) %>%
  summarize(total_alarm = sum(alarm)) %>% 
  group_by(Social_Index) %>% 
  mutate(proportion_alarm = total_alarm / sum(total_alarm))

view(social_group)

# multiply by proportions
percent_solo <- mean(hwk_subset$alarm[hwk_subset$Social_Index == 0], na.rm = TRUE) * 100
percent_pair <- mean(hwk_subset$alarm[hwk_subset$Social_Index == 1], na.rm = TRUE) * 100
percent_comp <- mean(hwk_subset$alarm[hwk_subset$Social_Index == 2], na.rm = TRUE) * 100
percent_ssf <- mean(hwk_subset$alarm[hwk_subset$Social_Index == 3], na.rm = TRUE) * 100
percent_msf <- mean(hwk_subset$alarm[hwk_subset$Social_Index == 4], na.rm = TRUE) * 100

```

## GLM - Social Index (factor)

```{r}
# subset to only hawk trials
hwk_subset <- subset(hwk_data, hwk == 1)

# model
model <- glm(Alarm ~ Social_Index, data = hwk_subset, family = binomial(link = "logit"))
summary(model)


```

## GLM - Social Index (numeric)

```{r}
# subset to only hawk trials
hwk_subset <- subset(hwk_data, hwk == 1)

# change Social_Index to numeric
social_numeric <- hwk_subset %>% 
  mutate(Social_Index = as.numeric(Social_Index))

# model
model <- glm(Alarm ~ Social_Index, data = social_numeric, family = binomial(link = "logit"))
summary(model)
```

## GLM - Social Index (factor), Competitive Removed

```{r}
# subset to only hawk trials
hwk_subset <- subset(hwk_data, hwk == 1)

# remove na from Social_Index
hwk_subset_narm <- hwk_subset[!is.na(hwk_subset$Social_Index), ]

# remove competitive
hwk_no_comp <- subset(hwk_subset_narm, Social_Index != 2)

# model
model <- glm(Alarm ~ Social_Index, data = hwk_no_comp, family = binomial(link = "logit"))
summary(model)
```

## GLM - Social Index (numeric), Competitive Removed

```{r}
# subset to only hawk trials
hwk_subset <- subset(hwk_data, hwk == 1)

# remove na from Social_Index
hwk_subset_narm <- hwk_subset[!is.na(hwk_subset$Social_Index), ]

# remove competitive
hwk_no_comp <- subset(hwk_subset_narm, Social_Index != 2)

# change Social_Index to numeric
social_numeric <- hwk_no_comp %>% 
  mutate(Social_Index = as.numeric(Social_Index))

# model
model <- glm(Alarm ~ Social_Index, data = social_numeric, family = binomial(link = "logit"))
summary(model)
```

## GLM - SI, Sex

```{r}
# subset to only hawk trials
hwk_subset <- subset(hwk_data, hwk == 1)
# str(hwk_subset)

# remove na from Social_Index
hwk_subset_narm <- hwk_subset[!is.na(hwk_subset$Social_Index), ]

# model
model <- glm(Alarm ~ Social_Index + sexID, data = hwk_subset_narm, family = binomial(link = "logit"))
summary(model)
```

```{r}
head(hwk_subset_narm)
```

## Remove Unwanted Variables

```{r}
# subset to only hawk trials
hwk_subset <- subset(hwk_data, hwk == 1)
str(hwk_subset)

# Columns to remove
unwanted_columns <- c("Timestamp_yymmdd_hhmmss","lat","long","obs1","shooter","Shoot_bird_dist","obs2","Obs2_bird_dist","Habitat_Primary","Habitat_Under_Dom1","Habitat_Under_Dom2","Basal Area 10m","Dbh_largest","Tar_Sp","Tar_Sp2","Species","BirdTree","Sex","Group_Type","Audio_React","hwk","msf","Forage_Tree_height","Rand_360_Degree","Rand_360_Dense_avg...35","Rand_360_Dense_avg...36","Pre_Behav","Pre_Substrate","Mid_Vocal","Mid_Vocal_Index","Mid_Behav","Mid_Behav_1","Dive","Hide_Freeze","Mid_Behav_Index","Mid_Behav_2","Mid_Join","Mid_Join_Index1","Mid_Join_Index2","Mid_Substrate","Post_Vocal","Post_Behav","Post_Substrate","Freeze_Time_sec","Post_Dist","Post_Bird_Dense_Avg","Post_Bird_Height","Veg_can_low","Veg_can_high","5m_dense","5mRad_01","5mRad_02","5mRad_03","5mRad_04","5mRad_05","5mRad_06","5mRad_07","5mRad_08","5mRad_09","5mRad_10")

# Remove unwanted columns using [ ] notation
df_subset <- hwk_subset[, !names(hwk_subset) %in% unwanted_columns]

# remove na
df_clean <- na.omit(df_subset)
```

## GLM - All Independent Vars (individual)

```{r}

# List of dependent variables
independent_vars <- c("Obs1_bird_dist","SppID","sexID","Flock_size","Social_Index","Flight_proximity","Pre_Height","Veg_dense_under_avg","Veg_under_height","Pre_Dense","Pre_Height","Veg_dense_under_avg","Veg_under_height","Pre_Dense","Veg_mid_low","Veg_mid_high","Rad_Avg_Dense")

# List to store model outputs
model_outputs <- list()

# Filter dataframe to include only rows where Pre_Height values are below 10
df_filtered <- df_numeric[df_numeric$Pre_Height < 10, ]

# Run GLM models
for (independent_var in independent_vars) {
  formula_string <- paste("Alarm ~ `", independent_var, "`", sep = "")
  model <- glm(as.formula(formula_string), data = df_filtered, family = binomial(link = "logit"))
  model_outputs[[independent_var]] <- broom::tidy(model)
}

# Combine model outputs into one large table
combined_table <- dplyr::bind_rows(model_outputs, .id = "Independent_Variable")

# Print the combined table
print(combined_table)

view(combined_table)
```

## GLM - All Independent Var (all combinations)

This takes A LOT of computing power

```{r}
# List of independent variables (ensure there are no leading or trailing spaces)
independent_vars <- c("Obs1_bird_dist", "SppID", "sexID", "Flock_size", "Social_Index", "Flight_proximity", 
                      "Pre_Height", "Veg_dense_under_avg", "Veg_under_height", "Pre_Dense", "Pre_Height", 
                      "Veg_dense_under_avg", "Veg_under_height", "Pre_Dense", "Veg_mid_low", "Veg_mid_high", 
                      "Rad_Avg_Dense")

# List to store model outputs
model_outputs <- list()

# Filter dataframe to include only rows where Pre_Height values are below 10
df_filtered <- df_numeric[df_numeric$Pre_Height < 10, ]

# Generate all combinations of independent variables
all_combinations <- lapply(seq_along(independent_vars), function(n) combn(independent_vars, n, simplify = FALSE))

# Flatten the list of combinations
all_combinations <- unlist(all_combinations, recursive = FALSE)

# Run GLM models for each combination
for (vars in all_combinations) {
  formula_string <- paste("Alarm ~", paste(vars, collapse = " + "))
  model <- glm(as.formula(formula_string), data = df_filtered, family = binomial(link = "logit"))
  model_outputs[[paste(vars, collapse = "_")]] <- broom::tidy(model)
}

# Combine model outputs into one large table
combined_table <- bind_rows(model_outputs, .id = "Combination")

# Print the combined table
print(combined_table)


view(combined_table)

# Define the file path using here()
file_path <- here("output", "all_vars_combined_table.csv")

# Write the combined table to the CSV file
write.csv(combined_table, file_path, row.names = FALSE)

```

## Social Index % by Group and Species

```{r}
# Convert "alarm" column to numeric
hwk_data$alarm <- as.numeric(hwk_data$alarm)

# Filter out non-numeric or missing values in the "alarm" column
hwk_data_filtered <- hwk_data %>%
  filter(!is.na(alarm)) %>%
  filter(!is.na(Social_Index)) %>%
  filter(is.numeric(alarm))

# Convert Social_Index to numeric
hwk_data_filtered$Social_Index <- as.numeric(as.character(hwk_data_filtered$Social_Index))

# Filter out rows where "Species" is exactly "#N/A_species"
hwk_data_filtered <- hwk_data_filtered %>%
  filter(Species != "#N/A_species")

# Calculate overall mean percentages by social structure
overall_means <- hwk_data_filtered %>%
  group_by(Social_Index) %>%
  summarize(Overall_Mean = mean(alarm, na.rm = TRUE) * 100)

# Calculate mean percentages by social structure and species
species_means <- hwk_data_filtered %>%
  group_by(Social_Index, Species) %>%
  summarize(Species_Mean = mean(alarm, na.rm = TRUE) * 100)

# Calculate mean percentages by unique species within each social index
unique_species_means <- hwk_data_filtered %>%
  group_by(Social_Index, Species) %>%
  summarize(Unique_Species_Mean = n() / length(unique(hwk_data_filtered$Trial_ID)) * 100)

# Order the levels of Social_Index
ordered_levels <- c(0, 1, 2, 3, 4)
overall_means$Social_Index <- factor(overall_means$Social_Index, levels = ordered_levels)
species_means$Social_Index <- factor(species_means$Social_Index, levels = ordered_levels)
unique_species_means$Social_Index <- factor(unique_species_means$Social_Index, levels = ordered_levels)

# Merge the data frames to ensure all levels are represented
combined_data <- merge(overall_means, species_means, by = c("Social_Index"))
combined_data <- merge(combined_data, unique_species_means, by = c("Social_Index", "Species"), all.x = TRUE)

# Create a stacked bar plot
ggplot(combined_data, aes(x = Social_Index, y = Unique_Species_Mean, fill = Species)) +
  geom_col(position = "fill", color = "white") +
  geom_text(aes(label = sprintf("%.1f%%", Unique_Species_Mean)), position = position_fill(vjust = 0.5)) +
  labs(title = "Stacked Bar Plot of Alarm Percentage by Social Index and Species",
       x = "Social Index",
       y = "Alarm Percentage") +
  scale_x_discrete(labels = c("0" = "Solo", "1" = "Pair", "2" = "Competitive", "3" = "SSF", "4" = "MSF")) +
  scale_y_continuous(labels = scales::percent_format(scale = 1), limits = c(0, 1)) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability

```

## Subset Lek Data

```{r}
hwk_lek <- subset(hwk_data, hwk_data$Social_Index == 2)
view(hwk_lek)
```

## GLM - Hwk Fake

find a package in r that's just for logistic regression

standard error may be asking what the differences are between the levels

Which one is different than the other ones – Least means, anova,

Figure out how to change the intercept for each index, p value is relative to the intercept, rotate each as if it's a zero in different colums

different types of test, whether there's an emperical control

post-hoc test tukey test maybe, compare each pair separately, general contrast approach (compare 2,3 to 1,4,5) or (1 to 2,3,4,)

These are the key predictions and hypotheses I have and how to test those, less than all pairwise and compare all groups. 1v2, 2v3, 3v4 etc

In a model fitting approach, i want to find which ones are the most influential (AIC or BIC)

```{r}

model_fake <- glm(Alarm ~ Social_Index, data = hwk_fake, family = binomial(link = "logit"))
summary(model_fake)
plot(model_fake)

nova <- anova(model_fake)
summary(nova)
```

figure out a code to establish factor levels Levels package?

## Social Gradient Fake Plot

```{r}
# Fake Data

# percentages of alarm by social structure
percent_solo <- mean(hwk_fake2$Alarm[hwk_fake$Social_Index == 0], na.rm = TRUE) * 100
percent_pair <- mean(hwk_fake2$Alarm[hwk_fake$Social_Index == 1], na.rm = TRUE) * 100
percent_comp <- mean(hwk_fake2$Alarm[hwk_fake$Social_Index == 2], na.rm = TRUE) * 100
percent_ssf <- mean(hwk_fake2$Alarm[hwk_fake$Social_Index == 3], na.rm = TRUE) * 100
percent_msf <- mean(hwk_fake2$Alarm[hwk_fake$Social_Index == 4], na.rm = TRUE) * 100

# Sample sizes
sample_solo <- sum(hwk_fake2$Social_Index == 0, na.rm = TRUE)
sample_pair <- sum(hwk_fake2$Social_Index == 1, na.rm = TRUE)
sample_comp <- sum(hwk_fake2$Social_Index == 2, na.rm = TRUE)
sample_ssf <- sum(hwk_fake2$Social_Index == 3, na.rm = TRUE)
sample_msf <- sum(hwk_fake2$Social_Index == 4, na.rm = TRUE)

# New dataframe for plotting
plot_data <- data.frame(
 Group = factor(c("Solo", "Pair", "Single-species Flock", "Mixed-species Flock", "Competitive"),
                levels = c("Solo", "Pair", "Single-species Flock", "Mixed-species Flock", "Competitive")),  
 Percentage = c(percent_solo, percent_pair, percent_ssf, percent_msf, percent_comp),
  SampleSize = c(sample_solo, sample_pair, sample_ssf, sample_msf, sample_comp)
)

# Create a bar plot
ggplot(plot_data, aes(x = Group, y = Percentage, fill = Group)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = paste("n =", SampleSize)), position = position_dodge(width = 0.9), vjust = -0.5) +
  labs(title = "Percentage of Positive Reactions by Sociality",
       y = "Alarm Percentage",
       x = "Group") +
  scale_y_continuous(labels = scales::percent_format(scale = 1), limits = c(0, 50)) +
  theme_minimal()
```

Query 1 million row data for max and min, parse criteria based on AIC value

need to do it relative to something, will generate a set of candidate models that will generate a range of best model fit of

one set of data for vegetation , one with social, one with both

fourth approach, run them both, figure out seem to be most important social and environmental, and then cross the most significant values from both and run it again

create linear regression for each predictor variable grouped by social index

## All Model Summary

```{r}
all_models <- read.csv(here("output/all_vars_combined_table.csv"))
summary(all_models)
head(all_models)
min(all_models$p.value)
```

## All Model Permutations with AIC

This takes a lot of time to run!!!! It runs through 317,000 combinations...and generates a csv with

```{r}
# List of independent variables (ensure there are no leading or trailing spaces)
independent_vars <- c("Obs1_bird_dist", "SppID", "sexID", "Flock_size", "Social_Index", "Flight_proximity", 
                      "Pre_Height", "Veg_dense_under_avg", "Veg_under_height", "Pre_Dense", "Pre_Height", 
                      "Veg_dense_under_avg", "Veg_under_height", "Pre_Dense", "Veg_mid_low", "Veg_mid_high", 
                      "Rad_Avg_Dense")

# List to store model outputs
model_outputs <- list()

# Filter dataframe to include only rows where Pre_Height values are below 10
df_filtered <- df_clean[df_clean$Pre_Height < 10, ]
str(df_filtered)
# Generate all combinations of independent variables
all_combinations <- lapply(seq_along(independent_vars), function(n) combn(independent_vars, n, simplify = FALSE))

# Flatten the list of combinations
all_combinations <- unlist(all_combinations, recursive = FALSE)

# Run GLM models for each combination
for (vars in all_combinations) {
  formula_string <- paste("Alarm ~", paste(vars, collapse = " + "))
  model <- glm(as.formula(formula_string), data = df_filtered, family = binomial(link = "logit"))
  # Extract AIC and other model output
  model_output <- broom::tidy(model)
  model_output$AIC <- AIC(model)
  # Store model output in list
  model_outputs[[paste(vars, collapse = "_")]] <- model_output
}

# Combine model outputs into one large table
combined_table <- bind_rows(model_outputs, .id = "Combination")

# Print the combined table
print(combined_table)


# Define the file path using here()
file_path <- here("output", "all_model_AIC_table.csv")

# Write the combined table to the CSV file
write.csv(combined_table, file_path, row.names = FALSE)
```

### Lowest AIC

```{r}
# Find the lowest AIC value
lowest_aic <- min(combined_table$AIC)

# Find the row(s) with AIC values within 2 units of the lowest AIC
best_AICs <- combined_table %>%
  filter(AIC <= lowest_aic + 2)

# Print the model(s) with AIC within 2 units of the lowest AIC
print(best_AICs)
view(best_AICs)
```

### Lowest P

```{r}
best_p <- combined_table %>%
  filter(p.value == min(p.value))
print(best_p)
```

```{r}
# Find the row(s) with the lowest p-value
best_models <- combined_table %>%
  filter(p.value == min(p.value))

# Extract all parameters of the model(s) with the lowest p-value
best_model_parameters <- lapply(best_models$Combination, function(combination) {
  model_output <- model_outputs[[combination]]
  return(model_output)
})

# Print the parameters of the model(s) with the lowest p-value
print(best_model_parameters)

```

### Top 100 p-values

```{r}

# Sort the combined table by p-value in ascending order
sorted_table <- combined_table %>%
  arrange(p.value)

# Select the top 100 rows with the lowest p-values
top_100_models <- head(sorted_table, 100)

# Extract all parameters of the top 100 models
top_100_model_parameters <- lapply(top_100_models$Combination, function(combination) {
  model_output <- model_outputs[[combination]]
  # Add combination column to retain information about the model combination
  model_output$Combination <- combination
  return(model_output)
})

# Combine all model parameters into one dataframe
combined_df <- bind_rows(top_100_model_parameters)

# Print the combined dataframe
print(combined_df)
view(combined_df)
```
